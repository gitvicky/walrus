_target_: walrus.trainer.Trainer
max_epoch: 200
val_frequency: 2
rollout_val_frequency: 5
short_validation_length: 20
max_rollout_steps: 60
num_time_intervals: 5
enable_amp: False
loss_fn:
  _target_: the_well.benchmark.metrics.NMAE
formatter:
  _target_: hydra.utils.get_class
  path: walrus.data.well_to_multi_transformer.ChannelsFirstWithTimeFormatter
revin:
  _target_: walrus.trainer.normalization_strat.MeanStdGlobalRevNormalization
  _partial_: True
prediction_type: delta
grad_acc_steps: 4
image_validation: True
video_validation: True
gradient_log_level: 0 # 0 is none, 1 is full norm only, higher not implemented
clip_gradient: 5.0
log_interval: 10 # How often to log training metrics to screen (causes GPU sync)
loss_multiplier: 1. # Multiply the loss by this factor so gradient scaling is easier
lr_scheduler_per_step: False
epsilon: 1e-10
validation_suite:
  # - _target_: the_well.benchmark.metrics.RMSE
  - _target_: the_well.benchmark.metrics.NRMSE
  # - _target_: the_well.benchmark.metrics.LInfinity
  - _target_: the_well.benchmark.metrics.VRMSE
  - _target_: the_well.benchmark.metrics.binned_spectral_mse
  - _target_: the_well.benchmark.metrics.PearsonR
# validation_trajectory_metrics:
#   - _target_: the_well.benchmark.metrics.HistogramW1
  # - _target_: the_well.benchmark.metrics.WindowedDTW
batch_aggregation_fns: # Note these are strings pointing to function to be resolved by hydra
  - torch.mean
  - torch.median
  - torch.std
