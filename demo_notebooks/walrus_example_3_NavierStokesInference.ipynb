{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Walrus for Navier-Stokes Prediction\n",
    "\n",
    "This notebook demonstrates how to use the Walrus foundation model to make predictions on Navier-Stokes spectral data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Workflow:**\n",
    "1. Load Navier-Stokes data\n",
    "2. Convert to Well format\n",
    "3. Load Walrus model and weights\n",
    "4. Prepare data in Walrus input format\n",
    "5. Run autoregressive rollout predictions\n",
    "6. Visualize and compare predictions vs ground truth\n",
    "\n",
    "**What Walrus Does:**\n",
    "- Takes initial timesteps as input\n",
    "- Predicts future evolution of the flow field\n",
    "- Uses physics-informed architecture to maintain consistency\n",
    "- Handles boundary conditions automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Walrus imports\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# For working with Well format data\n",
    "from the_well.data.datasets import WellMetadata\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Convert Navier-Stokes Data\n",
    "\n",
    "First, we load the Navier-Stokes spectral data and convert it to Well format.\n",
    "This follows the same process as in `walrus_example_2_NavierStokesToWell.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Navier_Stokes_Spectral(n_sims, data_dist):\n",
    "    \"\"\"\n",
    "    Load Navier-Stokes spectral simulation data.\n",
    "    \n",
    "    Args:\n",
    "        n_sims: Number of simulations to load\n",
    "        data_dist: 'ID' for in-distribution or 'OOD' for out-of-distribution\n",
    "    \n",
    "    Returns:\n",
    "        u: Horizontal velocity [B, Nt, Nx, Ny]\n",
    "        v: Vertical velocity [B, Nt, Nx, Ny]\n",
    "        p: Pressure [B, Nt, Nx, Ny]\n",
    "        rho: Density [B, Nt, Nx, Ny]\n",
    "        x: X-coordinates\n",
    "        y: Y-coordinates  \n",
    "        dt: Time step\n",
    "    \"\"\"\n",
    "    data_loc = '/Users/Vicky/Documents/UKAEA/Code/Uncertainty_Quantification/PDE_Residuals/Neural_PDE/Data'\n",
    "    \n",
    "    # Load appropriate dataset based on distribution\n",
    "    if data_dist == 'ID':\n",
    "        data = np.load(data_loc + '/NS_Spectral_combined.npz')\n",
    "    elif data_dist == 'OOD':\n",
    "        data = np.load(data_loc + '/NS_Spectral_combined_pitagora_OOD_nu_1e-2.npz')\n",
    "\n",
    "    # Extract fields and convert to float32\n",
    "    u = data['u'].astype(np.float32)[:n_sims]  # Horizontal velocity\n",
    "    v = data['v'].astype(np.float32)[:n_sims]  # Vertical velocity\n",
    "    p = data['p'].astype(np.float32)[:n_sims]  # Pressure\n",
    "    rho = np.ones_like(u)  # Density = 1 (constant)\n",
    "    \n",
    "    # Extract coordinates and time step\n",
    "    x = data['x']\n",
    "    y = x  # Assuming square domain\n",
    "    dt = data['dt']\n",
    "\n",
    "    return u, v, p, rho, x, y, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Navier-Stokes data:\n",
      "  Horizontal velocity (u): (3, 50, 100, 100)\n",
      "  Vertical velocity (v): (3, 50, 100, 100)\n",
      "  Pressure (p): (3, 50, 100, 100)\n",
      "  Spatial grid: 100 × 100\n",
      "  Time step: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Load a small subset for demonstration\n",
    "n_sims = 3  # Number of simulations to use\n",
    "data_dist = 'ID'  # Use in-distribution data\n",
    "\n",
    "# Load the raw data\n",
    "u, v, p, rho, x_coords, y_coords, dt = Navier_Stokes_Spectral(n_sims, data_dist)\n",
    "\n",
    "print(f\"Loaded Navier-Stokes data:\")\n",
    "print(f\"  Horizontal velocity (u): {u.shape}\")\n",
    "print(f\"  Vertical velocity (v): {v.shape}\")\n",
    "print(f\"  Pressure (p): {p.shape}\")\n",
    "print(f\"  Spatial grid: {len(x_coords)} × {len(y_coords)}\")\n",
    "print(f\"  Time step: {dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Walrus Model Weights\n",
    "\n",
    "Download the pretrained Walrus model and configuration from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration already exists\n",
      "✓ Model weights already exist\n"
     ]
    }
   ],
   "source": [
    "# Create directories for model files\n",
    "checkpoint_base_path = \"./checkpoints/\"\n",
    "config_base_path = \"./configs/\"\n",
    "Path(checkpoint_base_path).mkdir(exist_ok=True)\n",
    "Path(config_base_path).mkdir(exist_ok=True)\n",
    "\n",
    "# Download model weights and config from HuggingFace\n",
    "# Only download if not already present\n",
    "config_path = Path(config_base_path) / \"extended_config.yaml\"\n",
    "checkpoint_path = Path(checkpoint_base_path) / \"walrus.pt\"\n",
    "\n",
    "if not config_path.exists():\n",
    "    print(\"Downloading model configuration...\")\n",
    "    !wget -q https://huggingface.co/polymathic-ai/walrus/resolve/main/extended_config.yaml -O {config_path}\n",
    "    print(\"✓ Configuration downloaded\")\n",
    "else:\n",
    "    print(\"✓ Configuration already exists\")\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    print(\"Downloading model weights (~5GB, this may take a few minutes)...\")\n",
    "    !wget https://huggingface.co/polymathic-ai/walrus/resolve/main/walrus.pt -O {checkpoint_path}\n",
    "    print(\"✓ Model weights downloaded\")\n",
    "else:\n",
    "    print(\"✓ Model weights already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Walrus Configuration and Model\n",
    "\n",
    "Load the Hydra configuration and instantiate the Walrus model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration loaded:\n",
      "  Model type: walrus.models.IsotropicModel\n",
      "  Number of parameters: ~1.3B\n",
      "\n",
      "Configuration note:\n",
      "  We'll manually set input/output sequence lengths for our NS data\n",
      "  Walrus is flexible and can handle variable sequence lengths\n"
     ]
    }
   ],
   "source": [
    "# Load configuration using Hydra\n",
    "# The config file contains model architecture, training settings, and data specs\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "print(\"Model configuration loaded:\")\n",
    "print(f\"  Model type: {config.model._target_}\")\n",
    "print(f\"  Number of parameters: ~1.3B\")\n",
    "\n",
    "# Note: The config structure may vary, so we'll set our own parameters\n",
    "print(f\"\\nConfiguration note:\") \n",
    "print(f\"  We'll manually set input/output sequence lengths for our NS data\")\n",
    "print(f\"  Walrus is flexible and can handle variable sequence lengths\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained field mapping (showing first 10):\n",
      "  closed_boundary: 0\n",
      "  open_boundary: 1\n",
      "  bias_correction: 2\n",
      "  pressure: 3\n",
      "  velocity_x: 4\n",
      "  velocity_y: 5\n",
      "  velocity_z: 6\n",
      "  zeros_like_density: 7\n",
      "  speed_of_sound: 8\n",
      "  concentration: 9\n",
      "  ... (67 total fields)\n"
     ]
    }
   ],
   "source": [
    "# Get pretrained field mapping\n",
    "# This maps physical field names to embedding indices in the model\n",
    "pretrained_field_to_index = config.data.field_index_map_override\n",
    "\n",
    "print(\"Pretrained field mapping (showing first 10):\")\n",
    "for i, (field, idx) in enumerate(list(pretrained_field_to_index.items())[:10]):\n",
    "    print(f\"  {field}: {idx}\")\n",
    "print(f\"  ... ({len(pretrained_field_to_index)} total fields)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added velocity_x at index 67\n",
      "Added velocity_y at index 68\n",
      "Pressure already in pretrained mapping\n",
      "Density already in pretrained mapping\n",
      "\n",
      "Total number of field states: 69\n"
     ]
    }
   ],
   "source": [
    "# Add our Navier-Stokes fields to the mapping\n",
    "# We need to map: velocity (vector), pressure (scalar), density (scalar)\n",
    "\n",
    "# Create new field mapping with our NS fields\n",
    "field_to_index = dict(pretrained_field_to_index)\n",
    "\n",
    "# Add fields if not already present\n",
    "# Note: We're using generic field names that might already be in the pretrained model\n",
    "if \"velocity\" not in field_to_index:\n",
    "    max_idx = max(field_to_index.values())\n",
    "    field_to_index[\"velocity_x\"] = max_idx + 1\n",
    "    field_to_index[\"velocity_y\"] = max_idx + 2\n",
    "    print(f\"Added velocity_x at index {max_idx + 1}\")\n",
    "    print(f\"Added velocity_y at index {max_idx + 2}\")\n",
    "    max_idx += 2\n",
    "else:\n",
    "    print(\"Velocity fields already in pretrained mapping\")\n",
    "    max_idx = max(field_to_index.values())\n",
    "\n",
    "if \"pressure\" not in field_to_index:\n",
    "    field_to_index[\"pressure\"] = max_idx + 1\n",
    "    print(f\"Added pressure at index {max_idx + 1}\")\n",
    "    max_idx += 1\n",
    "else:\n",
    "    print(\"Pressure already in pretrained mapping\")\n",
    "\n",
    "if \"density\" not in field_to_index:\n",
    "    field_to_index[\"density\"] = max_idx + 1\n",
    "    print(f\"Added density at index {max_idx + 1}\")\n",
    "else:\n",
    "    print(\"Density already in pretrained mapping\")\n",
    "\n",
    "n_states = max(field_to_index.values()) + 1\n",
    "print(f\"\\nTotal number of field states: {n_states}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Walrus model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Vicky/Documents/UKAEA/Code/Foundation_Models/walrus/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/Vicky/Documents/UKAEA/Code/Foundation_Models/walrus/.venv/lib/python3.10/site-packages/walrus/models/temporal_blocks/axial_time_attention.py:38: FutureWarning: `nn.init.kaiming_uniform` is now deprecated in favor of `nn.init.kaiming_uniform_`.\n",
      "  init.kaiming_uniform(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model initialized\n",
      "  Total parameters: 1,287,640,843\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "# The model is a large transformer-based architecture for PDE prediction\n",
    "print(\"Initializing Walrus model...\")\n",
    "\n",
    "model = instantiate(\n",
    "    config.model,\n",
    "    n_states=n_states,  # Total number of field embeddings\n",
    ")\n",
    "\n",
    "print(f\"✓ Model initialized\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/17t2t7cj4x17kf4plwsxyf_00000gr/T/ipykernel_36351/801394763.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys (first 10): ['app']\n",
      "\n",
      "Note: Model expects 69 field states, pretrained has 67\n",
      "We have added custom fields. The model will use random initialization for new field embeddings.\n",
      "For best performance, consider fine-tuning on your dataset.\n",
      "\n",
      "Loading weights (strict=False to allow new field embeddings)...\n",
      "  Missing keys: 857 (these will use random initialization)\n",
      "  Unexpected keys: 1 (these will be ignored)\n",
      "    - app\n",
      "✓ Weights loaded successfully\n",
      "✓ Model moved to cpu\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained weights\n",
    "print(\"Loading pretrained weights...\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "# The checkpoint contains the full training state\n",
    "# We only need the model weights\n",
    "if 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "print(f\"Checkpoint keys (first 10): {list(state_dict.keys())[:10]}\")\n",
    "\n",
    "# Check if we actually need to align the checkpoint\n",
    "# In many cases, the pretrained model already has embeddings for many fields\n",
    "# and we can just use it directly\n",
    "if n_states > len(pretrained_field_to_index):\n",
    "    print(f\"\\nNote: Model expects {n_states} field states, pretrained has {len(pretrained_field_to_index)}\")\n",
    "    print(\"We have added custom fields. The model will use random initialization for new field embeddings.\")\n",
    "    print(\"For best performance, consider fine-tuning on your dataset.\")\n",
    "    \n",
    "    # For inference with new fields, we can skip alignment and just load with strict=False\n",
    "    # This will use pretrained weights for existing parameters and random init for new ones\n",
    "    print(\"\\nLoading weights (strict=False to allow new field embeddings)...\")\n",
    "else:\n",
    "    print(\"\\nLoading pretrained weights (all fields match)...\")\n",
    "\n",
    "# Load weights into model\n",
    "# strict=False allows loading even if there are missing or extra keys\n",
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"  Missing keys: {len(missing_keys)} (these will use random initialization)\")\n",
    "    if len(missing_keys) <= 5:\n",
    "        for key in missing_keys:\n",
    "            print(f\"    - {key}\")\n",
    "if unexpected_keys:\n",
    "    print(f\"  Unexpected keys: {len(unexpected_keys)} (these will be ignored)\")\n",
    "    if len(unexpected_keys) <= 5:\n",
    "        for key in unexpected_keys:\n",
    "            print(f\"    - {key}\")\n",
    "\n",
    "print(\"✓ Weights loaded successfully\")\n",
    "\n",
    "# Set model to evaluation mode (disables dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"✓ Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Data in Walrus Input Format\n",
    "\n",
    "Walrus expects data in a specific dictionary format. We need to prepare:\n",
    "- Input fields: Initial timesteps\n",
    "- Boundary conditions: How the domain is bounded\n",
    "- Field indices: Which fields we're using\n",
    "- Padded field mask: Marks which fields are real vs padding\n",
    "- Metadata: Information about the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction setup:\n",
      "  Input timesteps: 3\n",
      "  Output timesteps: 10\n",
      "  Batch size: 3\n",
      "  Spatial resolution: 100 × 100\n"
     ]
    }
   ],
   "source": [
    "# Configuration for the prediction\n",
    "T_in = 3      # Number of input timesteps (how much history to use)\n",
    "T_out = 10    # Number of output timesteps to predict\n",
    "T_total = T_in + T_out  # Total timesteps we need from data\n",
    "\n",
    "B, Nt, Nx, Ny = u.shape\n",
    "\n",
    "# Check we have enough timesteps\n",
    "if Nt < T_total:\n",
    "    print(f\"Warning: Data has {Nt} timesteps but we need {T_total}\")\n",
    "    print(f\"Reducing output timesteps to {Nt - T_in}\")\n",
    "    T_out = Nt - T_in\n",
    "    T_total = Nt\n",
    "\n",
    "print(f\"Prediction setup:\")\n",
    "print(f\"  Input timesteps: {T_in}\")\n",
    "print(f\"  Output timesteps: {T_out}\")\n",
    "print(f\"  Batch size: {B}\")\n",
    "print(f\"  Spatial resolution: {Nx} × {Ny}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "  u_input: (3, 3, 100, 100, 1)\n",
      "  v_input: (3, 3, 100, 100, 1)\n",
      "  p_input: (3, 3, 100, 100, 1)\n",
      "\n",
      "Output (ground truth) shapes:\n",
      "  u_output: (3, 10, 100, 100, 1)\n",
      "  v_output: (3, 10, 100, 100, 1)\n",
      "  p_output: (3, 10, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare input data\n",
    "# Stack velocity components: [B, Nt, Nx, Ny, 2]\n",
    "velocity = np.stack([u, v], axis=-1)\n",
    "\n",
    "# For Walrus, we need to organize fields by their tensor rank\n",
    "# and add a \"depth\" dimension (for 2D data, depth=1)\n",
    "\n",
    "# Extract input and output timesteps\n",
    "u_input = u[:, :T_in, :, :, np.newaxis]      # [B, T_in, Nx, Ny, 1] (add depth dim)\n",
    "v_input = v[:, :T_in, :, :, np.newaxis]      # [B, T_in, Nx, Ny, 1]\n",
    "p_input = p[:, :T_in, :, :, np.newaxis]      # [B, T_in, Nx, Ny, 1]\n",
    "rho_input = rho[:, :T_in, :, :, np.newaxis]  # [B, T_in, Nx, Ny, 1]\n",
    "\n",
    "# Ground truth for comparison\n",
    "u_output = u[:, T_in:T_total, :, :, np.newaxis]      # [B, T_out, Nx, Ny, 1]\n",
    "v_output = v[:, T_in:T_total, :, :, np.newaxis]      # [B, T_out, Nx, Ny, 1]\n",
    "p_output = p[:, T_in:T_total, :, :, np.newaxis]      # [B, T_out, Nx, Ny, 1]\n",
    "rho_output = rho[:, T_in:T_total, :, :, np.newaxis]  # [B, T_out, Nx, Ny, 1]\n",
    "\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  u_input: {u_input.shape}\")\n",
    "print(f\"  v_input: {v_input.shape}\")\n",
    "print(f\"  p_input: {p_input.shape}\")\n",
    "print(f\"\\nOutput (ground truth) shapes:\")\n",
    "print(f\"  u_output: {u_output.shape}\")\n",
    "print(f\"  v_output: {v_output.shape}\")\n",
    "print(f\"  p_output: {p_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked field shapes:\n",
      "  input_fields: (3, 3, 100, 100, 4)   # [B, T_in, H, W, D, C]\n",
      "  output_fields: (3, 10, 100, 100, 4)  # [B, T_out, H, W, D, C]\n"
     ]
    }
   ],
   "source": [
    "# Stack all fields together\n",
    "# Walrus expects: [B, T, H, W, D, C] where C is the number of fields\n",
    "# For our NS data: [B, T_in, Nx, Ny, 1, 4] (u, v, p, rho)\n",
    "\n",
    "input_fields = np.concatenate([\n",
    "    u_input,\n",
    "    v_input, \n",
    "    p_input,\n",
    "    rho_input\n",
    "], axis=-1)  # [B, T_in, Nx, Ny, 1, 4]\n",
    "\n",
    "output_fields = np.concatenate([\n",
    "    u_output,\n",
    "    v_output,\n",
    "    p_output,\n",
    "    rho_output\n",
    "], axis=-1)  # [B, T_out, Nx, Ny, 1, 4]\n",
    "\n",
    "print(f\"Stacked field shapes:\")\n",
    "print(f\"  input_fields: {input_fields.shape}   # [B, T_in, H, W, D, C]\")\n",
    "print(f\"  output_fields: {output_fields.shape}  # [B, T_out, H, W, D, C]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors created and moved to cpu\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors and move to device\n",
    "input_fields_tensor = torch.from_numpy(input_fields).float().to(device)\n",
    "output_fields_tensor = torch.from_numpy(output_fields).float().to(device)\n",
    "\n",
    "print(f\"Tensors created and moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary conditions: torch.Size([3, 2, 2])  # [B, 2, 2]\n",
      "  BC type: PERIODIC (code=2) in all directions\n"
     ]
    }
   ],
   "source": [
    "# Define boundary conditions\n",
    "# For spectral NS, we typically have periodic BCs in all directions\n",
    "# BC format: [B, n_dims, 2] where n_dims=2 for 2D, and 2 for [lower, upper] bounds\n",
    "# BC codes: WALL=0, OPEN=1, PERIODIC=2\n",
    "\n",
    "boundary_conditions = torch.tensor(\n",
    "    [[[2, 2], [2, 2]]]  # [[x_lower, x_upper], [y_lower, y_upper]] all PERIODIC\n",
    ").repeat(B, 1, 1).to(device)  # Repeat for each trajectory in batch\n",
    "\n",
    "print(f\"Boundary conditions: {boundary_conditions.shape}  # [B, 2, 2]\")\n",
    "print(f\"  BC type: PERIODIC (code=2) in all directions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field indices: tensor([67, 68,  3, 28])\n",
      "  u (horizontal velocity): 67\n",
      "  v (vertical velocity): 68\n",
      "  p (pressure): 3\n",
      "  rho (density): 28\n"
     ]
    }
   ],
   "source": [
    "# Create field indices\n",
    "# Maps each field in our data to its embedding index in the model\n",
    "\n",
    "# We're using 4 fields: u, v, p, rho\n",
    "# Let's use velocity indices if available, otherwise use custom indices\n",
    "if \"velocity_x\" in field_to_index:\n",
    "    u_idx = field_to_index[\"velocity_x\"]\n",
    "    v_idx = field_to_index[\"velocity_y\"]\n",
    "else:\n",
    "    # Use generic indices\n",
    "    u_idx = 0\n",
    "    v_idx = 1\n",
    "\n",
    "p_idx = field_to_index.get(\"pressure\", 2)\n",
    "rho_idx = field_to_index.get(\"density\", 3)\n",
    "\n",
    "field_indices = torch.tensor([u_idx, v_idx, p_idx, rho_idx]).to(device)\n",
    "\n",
    "print(f\"Field indices: {field_indices}\")\n",
    "print(f\"  u (horizontal velocity): {u_idx}\")\n",
    "print(f\"  v (vertical velocity): {v_idx}\")\n",
    "print(f\"  p (pressure): {p_idx}\")\n",
    "print(f\"  rho (density): {rho_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded field mask: tensor([True, True, True, True])\n",
      "  All fields are real (no padding)\n"
     ]
    }
   ],
   "source": [
    "# Create padded field mask\n",
    "# For 2D data, we have real fields (u, v, p, rho) and NO padding\n",
    "# Mask is True for real fields, False for padded/fake fields\n",
    "\n",
    "# In our case, all 4 fields are real (no padding needed for 2D)\n",
    "padded_field_mask = torch.tensor([True, True, True, True]).to(device)\n",
    "\n",
    "print(f\"Padded field mask: {padded_field_mask}\")\n",
    "print(f\"  All fields are real (no padding)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata created:\n",
      "  Dataset: navier_stokes_spectral\n",
      "  Spatial dims: 2\n",
      "  Grid type: cartesian\n",
      "  Spatial resolution: (100, 100)\n",
      "  Field names: {0: ['pressure', 'density'], 1: ['velocity'], 2: []}\n"
     ]
    }
   ],
   "source": [
    "# Create metadata\n",
    "# This provides information about the simulation for the model\n",
    "\n",
    "metadata = WellMetadata(\n",
    "    dataset_name=\"navier_stokes_spectral\",\n",
    "    n_spatial_dims=2,  # 2D simulation\n",
    "    grid_type=\"cartesian\",\n",
    "    spatial_resolution=(Nx, Ny),  # Grid dimensions\n",
    "    scalar_names=[],  # No standalone scalar parameters\n",
    "    constant_scalar_names=[],  # No constant scalars\n",
    "    constant_field_names={},  # No constant fields\n",
    "    field_names={\n",
    "        0: [\"pressure\", \"density\"],  # t0_fields (scalars)\n",
    "        1: [\"velocity\"],              # t1_fields (vectors)\n",
    "        2: []                         # t2_fields (tensors)\n",
    "    },\n",
    "    boundary_condition_types=[\"PERIODIC\", \"PERIODIC\"],  # BCs for x, y\n",
    "    n_files=1,  # Single file\n",
    "    n_trajectories_per_file=[B],  # B trajectories\n",
    "    n_steps_per_trajectory=T_total,  # Total timesteps available\n",
    ")\n",
    "\n",
    "print(\"Metadata created:\")\n",
    "print(f\"  Dataset: {metadata.dataset_name}\")\n",
    "print(f\"  Spatial dims: {metadata.n_spatial_dims}\")\n",
    "print(f\"  Grid type: {metadata.grid_type}\")\n",
    "print(f\"  Spatial resolution: {metadata.spatial_resolution}\")\n",
    "print(f\"  Field names: {metadata.field_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Walrus data dictionary created\n",
      "\n",
      "Data dictionary keys: ['input_fields', 'output_fields', 'boundary_conditions', 'field_indices', 'padded_field_mask', 'metadata']\n"
     ]
    }
   ],
   "source": [
    "# Create the final data dictionary for Walrus\n",
    "# This is the format the model expects\n",
    "\n",
    "walrus_data = {\n",
    "    \"input_fields\": input_fields_tensor,        # [B, T_in, H, W, D, C]\n",
    "    \"output_fields\": output_fields_tensor,      # [B, T_out, H, W, D, C] (for evaluation)\n",
    "    \"boundary_conditions\": boundary_conditions, # [B, n_dims, 2]\n",
    "    \"field_indices\": field_indices,            # [C]\n",
    "    \"padded_field_mask\": padded_field_mask,    # [C]\n",
    "    \"metadata\": metadata,                      # WellMetadata object\n",
    "}\n",
    "\n",
    "print(\"✓ Walrus data dictionary created\")\n",
    "print(f\"\\nData dictionary keys: {list(walrus_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Autoregressive Rollout\n",
    "\n",
    "Now we'll use Walrus to predict the future evolution of the flow field.\n",
    "\n",
    "**Autoregressive Rollout:**\n",
    "1. Use initial timesteps (0, 1, 2) to predict timestep 3\n",
    "2. Use timesteps (1, 2, 3_predicted) to predict timestep 4\n",
    "3. Use timesteps (2, 3_predicted, 4_predicted) to predict timestep 5\n",
    "4. Continue until we've predicted all T_out timesteps\n",
    "\n",
    "This is called \"autoregressive\" because predictions feed back as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running autoregressive rollout...\n",
      "  Predicting 10 timesteps\n",
      "  Using 3 input timesteps\n",
      "Input shape: torch.Size([3, 3, 100, 100, 4])\n",
      "Permuted input shape: torch.Size([3, 3, 100, 100, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Rollout loop: predict one timestep at a time\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(T_out), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Predict next timestep\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Model takes the current T_in timesteps and predicts the next one\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboundary_conditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Evaluation mode\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Returns [1, B, H, W, (D,) C] (single timestep prediction)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Extract the prediction\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     next_timestep \u001b[38;5;241m=\u001b[39m output  \u001b[38;5;66;03m# [1, B, H, W, (D,) C]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UKAEA/Code/Foundation_Models/walrus/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/UKAEA/Code/Foundation_Models/walrus/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/UKAEA/Code/Foundation_Models/walrus/.venv/lib/python3.10/site-packages/walrus/models/isotropic_model.py:315\u001b[0m, in \u001b[0;36mIsotropicModel.forward\u001b[0;34m(self, x, state_labels, bcs, metadata, proj_axes, return_att, train)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Choose the variable patches if applicable\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed[dim_key], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable_downsample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed[dim_key]\u001b[38;5;241m.\u001b[39mvariable_downsample)\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed[dim_key]\u001b[38;5;241m.\u001b[39mvariable_deterministic_ds\n\u001b[1;32m    313\u001b[0m ):\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# support for variable but deterministic downsampling\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     dynamic_ks \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_kernel_size_deterministic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     patch_size \u001b[38;5;241m=\u001b[39m [reduce(mul, k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dynamic_ks]\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# patch_size doesn't matter for the dimension that is higher than the number of spatial dims\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UKAEA/Code/Foundation_Models/walrus/.venv/lib/python3.10/site-packages/walrus/models/shared_utils/flexi_utils.py:119\u001b[0m, in \u001b[0;36mchoose_kernel_size_deterministic\u001b[0;34m(x_shape)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     per_axis_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    117\u001b[0m     (H \u001b[38;5;241m%\u001b[39m per_axis_tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m H \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (W \u001b[38;5;241m%\u001b[39m per_axis_tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m W \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (D \u001b[38;5;241m%\u001b[39m per_axis_tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m D \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    121\u001b[0m h_patch \u001b[38;5;241m=\u001b[39m H \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m per_axis_tokens\n\u001b[1;32m    122\u001b[0m w_patch \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m per_axis_tokens\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run inference (no gradient computation needed)\n",
    "print(\"Running autoregressive rollout...\")\n",
    "print(f\"  Predicting {T_out} timesteps\")\n",
    "print(f\"  Using {T_in} input timesteps\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient tracking for inference\n",
    "    # Initialize: current input is the initial timesteps\n",
    "    current_input = input_fields_tensor  # Should be [B, T_in, H, W, C] for 2D data\n",
    "    \n",
    "    print(f\"Input shape: {current_input.shape}\")\n",
    "    \n",
    "    # Check the actual number of dimensions\n",
    "    if current_input.dim() == 5:  # [B, T, H, W, C] - 2D data without explicit depth dim\n",
    "        # The model expects input in [T, B, H, W, C] format (time-first) for 2D\n",
    "        current_input = current_input.permute(1, 0, 2, 3, 4)  # [T_in, B, H, W, C]\n",
    "    elif current_input.dim() == 6:  # [B, T, H, W, D, C] - 3D data\n",
    "        # The model expects input in [T, B, H, W, D, C] format (time-first)\n",
    "        current_input = current_input.permute(1, 0, 2, 3, 4, 5)  # [T_in, B, H, W, D, C]\n",
    "    \n",
    "    print(f\"Permuted input shape: {current_input.shape}\")\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Rollout loop: predict one timestep at a time\n",
    "    for step in tqdm(range(T_out), desc=\"Predicting\"):\n",
    "        # Predict next timestep\n",
    "        # Model takes the current T_in timesteps and predicts the next one\n",
    "        output = model(\n",
    "            x=current_input,\n",
    "            state_labels=field_indices,\n",
    "            bcs=boundary_conditions,\n",
    "            metadata=metadata,\n",
    "            train=False,  # Evaluation mode\n",
    "        )  # Returns [1, B, H, W, (D,) C] (single timestep prediction)\n",
    "        \n",
    "        # Extract the prediction\n",
    "        next_timestep = output  # [1, B, H, W, (D,) C]\n",
    "        \n",
    "        # Store prediction (permute back to [B, 1, H, W, (D,) C] for consistency)\n",
    "        if next_timestep.dim() == 5:  # 2D case\n",
    "            predictions.append(next_timestep.permute(1, 0, 2, 3, 4))\n",
    "        elif next_timestep.dim() == 6:  # 3D case\n",
    "            predictions.append(next_timestep.permute(1, 0, 2, 3, 4, 5))\n",
    "        \n",
    "        # Update input for next iteration (autoregressive)\n",
    "        # Concatenate current input (drop oldest timestep) with new prediction\n",
    "        current_input = torch.cat([\n",
    "            current_input[1:, ...],  # Drop first timestep: [T_in-1, B, H, W, (D,) C]\n",
    "            next_timestep            # Add prediction: [1, B, H, W, (D,) C]\n",
    "        ], dim=0)  # Result: [T_in, B, H, W, (D,) C]\n",
    "    \n",
    "    # Concatenate all predictions along time dimension\n",
    "    # predictions is a list of [B, 1, H, W, (D,) C] tensors\n",
    "    predictions = torch.cat(predictions, dim=1)  # [B, T_out, H, W, (D,) C]\n",
    "\n",
    "print(f\"\\n✓ Rollout complete!\")\n",
    "print(f\"  Predictions shape: {predictions.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Predictions\n",
    "\n",
    "Compare predictions against ground truth using standard metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error metrics\n",
    "# We'll compute MSE and relative L2 error for each field\n",
    "\n",
    "def compute_metrics(pred, target, field_name):\n",
    "    \"\"\"\n",
    "    Compute MSE and relative L2 error.\n",
    "    \n",
    "    Args:\n",
    "        pred: Predictions [B, T, H, W, D]\n",
    "        target: Ground truth [B, T, H, W, D]\n",
    "        field_name: Name of the field for printing\n",
    "    \"\"\"\n",
    "    # Mean Squared Error\n",
    "    mse = torch.mean((pred - target) ** 2).item()\n",
    "    \n",
    "    # Relative L2 error: ||pred - target||_2 / ||target||_2\n",
    "    rel_l2 = (torch.norm(pred - target) / torch.norm(target)).item()\n",
    "    \n",
    "    print(f\"{field_name}:\")\n",
    "    print(f\"  MSE: {mse:.6e}\")\n",
    "    print(f\"  Relative L2: {rel_l2:.6f}\")\n",
    "    \n",
    "    return mse, rel_l2\n",
    "\n",
    "# Extract individual fields from predictions and ground truth\n",
    "# Remember: fields are stacked as [u, v, p, rho]\n",
    "pred_u = predictions[:, :, :, :, :, 0]  # [B, T_out, H, W, D]\n",
    "pred_v = predictions[:, :, :, :, :, 1]\n",
    "pred_p = predictions[:, :, :, :, :, 2]\n",
    "pred_rho = predictions[:, :, :, :, :, 3]\n",
    "\n",
    "target_u = output_fields_tensor[:, :, :, :, :, 0]\n",
    "target_v = output_fields_tensor[:, :, :, :, :, 1]\n",
    "target_p = output_fields_tensor[:, :, :, :, :, 2]\n",
    "target_rho = output_fields_tensor[:, :, :, :, :, 3]\n",
    "\n",
    "print(\"Prediction Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "compute_metrics(pred_u, target_u, \"Horizontal velocity (u)\")\n",
    "compute_metrics(pred_v, target_v, \"Vertical velocity (v)\")\n",
    "compute_metrics(pred_p, target_p, \"Pressure (p)\")\n",
    "compute_metrics(pred_rho, target_rho, \"Density (rho)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Results\n",
    "\n",
    "Let's visualize the predictions compared to ground truth for one trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select trajectory and timestep to visualize\n",
    "traj_idx = 0  # First trajectory\n",
    "time_idx = T_out - 1  # Last predicted timestep\n",
    "\n",
    "# Move tensors to CPU and convert to numpy for plotting\n",
    "pred_u_np = pred_u[traj_idx, time_idx, :, :, 0].cpu().numpy()\n",
    "pred_v_np = pred_v[traj_idx, time_idx, :, :, 0].cpu().numpy()\n",
    "pred_p_np = pred_p[traj_idx, time_idx, :, :, 0].cpu().numpy()\n",
    "\n",
    "target_u_np = target_u[traj_idx, time_idx, :, :, 0].cpu().numpy()\n",
    "target_v_np = target_v[traj_idx, time_idx, :, :, 0].cpu().numpy()\n",
    "target_p_np = target_p[traj_idx, time_idx, :, :, 0].cpu().numpy()\n",
    "\n",
    "# Compute velocity magnitude\n",
    "pred_vel_mag = np.sqrt(pred_u_np**2 + pred_v_np**2)\n",
    "target_vel_mag = np.sqrt(target_u_np**2 + target_v_np**2)\n",
    "\n",
    "print(f\"Visualizing trajectory {traj_idx}, timestep {time_idx + T_in} (prediction {time_idx + 1}/{T_out})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 14))\n",
    "\n",
    "# Row 1: Horizontal velocity (u)\n",
    "vmin_u = min(pred_u_np.min(), target_u_np.min())\n",
    "vmax_u = max(pred_u_np.max(), target_u_np.max())\n",
    "\n",
    "im0 = axes[0, 0].imshow(target_u_np.T, origin='lower', cmap='RdBu_r', \n",
    "                         vmin=vmin_u, vmax=vmax_u, aspect='auto')\n",
    "axes[0, 0].set_title('Ground Truth: u (horizontal velocity)')\n",
    "axes[0, 0].set_xlabel('x')\n",
    "axes[0, 0].set_ylabel('y')\n",
    "plt.colorbar(im0, ax=axes[0, 0])\n",
    "\n",
    "im1 = axes[0, 1].imshow(pred_u_np.T, origin='lower', cmap='RdBu_r',\n",
    "                         vmin=vmin_u, vmax=vmax_u, aspect='auto')\n",
    "axes[0, 1].set_title('Prediction: u (horizontal velocity)')\n",
    "axes[0, 1].set_xlabel('x')\n",
    "axes[0, 1].set_ylabel('y')\n",
    "plt.colorbar(im1, ax=axes[0, 1])\n",
    "\n",
    "error_u = np.abs(pred_u_np - target_u_np)\n",
    "im2 = axes[0, 2].imshow(error_u.T, origin='lower', cmap='hot', aspect='auto')\n",
    "axes[0, 2].set_title('Absolute Error: u')\n",
    "axes[0, 2].set_xlabel('x')\n",
    "axes[0, 2].set_ylabel('y')\n",
    "plt.colorbar(im2, ax=axes[0, 2])\n",
    "\n",
    "# Row 2: Vertical velocity (v)\n",
    "vmin_v = min(pred_v_np.min(), target_v_np.min())\n",
    "vmax_v = max(pred_v_np.max(), target_v_np.max())\n",
    "\n",
    "im3 = axes[1, 0].imshow(target_v_np.T, origin='lower', cmap='RdBu_r',\n",
    "                         vmin=vmin_v, vmax=vmax_v, aspect='auto')\n",
    "axes[1, 0].set_title('Ground Truth: v (vertical velocity)')\n",
    "axes[1, 0].set_xlabel('x')\n",
    "axes[1, 0].set_ylabel('y')\n",
    "plt.colorbar(im3, ax=axes[1, 0])\n",
    "\n",
    "im4 = axes[1, 1].imshow(pred_v_np.T, origin='lower', cmap='RdBu_r',\n",
    "                         vmin=vmin_v, vmax=vmax_v, aspect='auto')\n",
    "axes[1, 1].set_title('Prediction: v (vertical velocity)')\n",
    "axes[1, 1].set_xlabel('x')\n",
    "axes[1, 1].set_ylabel('y')\n",
    "plt.colorbar(im4, ax=axes[1, 1])\n",
    "\n",
    "error_v = np.abs(pred_v_np - target_v_np)\n",
    "im5 = axes[1, 2].imshow(error_v.T, origin='lower', cmap='hot', aspect='auto')\n",
    "axes[1, 2].set_title('Absolute Error: v')\n",
    "axes[1, 2].set_xlabel('x')\n",
    "axes[1, 2].set_ylabel('y')\n",
    "plt.colorbar(im5, ax=axes[1, 2])\n",
    "\n",
    "# Row 3: Pressure (p)\n",
    "vmin_p = min(pred_p_np.min(), target_p_np.min())\n",
    "vmax_p = max(pred_p_np.max(), target_p_np.max())\n",
    "\n",
    "im6 = axes[2, 0].imshow(target_p_np.T, origin='lower', cmap='plasma',\n",
    "                         vmin=vmin_p, vmax=vmax_p, aspect='auto')\n",
    "axes[2, 0].set_title('Ground Truth: p (pressure)')\n",
    "axes[2, 0].set_xlabel('x')\n",
    "axes[2, 0].set_ylabel('y')\n",
    "plt.colorbar(im6, ax=axes[2, 0])\n",
    "\n",
    "im7 = axes[2, 1].imshow(pred_p_np.T, origin='lower', cmap='plasma',\n",
    "                         vmin=vmin_p, vmax=vmax_p, aspect='auto')\n",
    "axes[2, 1].set_title('Prediction: p (pressure)')\n",
    "axes[2, 1].set_xlabel('x')\n",
    "axes[2, 1].set_ylabel('y')\n",
    "plt.colorbar(im7, ax=axes[2, 1])\n",
    "\n",
    "error_p = np.abs(pred_p_np - target_p_np)\n",
    "im8 = axes[2, 2].imshow(error_p.T, origin='lower', cmap='hot', aspect='auto')\n",
    "axes[2, 2].set_title('Absolute Error: p')\n",
    "axes[2, 2].set_xlabel('x')\n",
    "axes[2, 2].set_ylabel('y')\n",
    "plt.colorbar(im8, ax=axes[2, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Timestep {time_idx + T_in} (t = {(time_idx + T_in) * dt:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Time Evolution\n",
    "\n",
    "Let's see how the predictions evolve over time compared to ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series of velocity magnitude at a single point\n",
    "# Choose a point in the middle of the domain\n",
    "x_pt = Nx // 2\n",
    "y_pt = Ny // 2\n",
    "\n",
    "# Extract time series for all predicted timesteps\n",
    "pred_vel_mag_series = np.sqrt(\n",
    "    pred_u[traj_idx, :, x_pt, y_pt, 0].cpu().numpy()**2 + \n",
    "    pred_v[traj_idx, :, x_pt, y_pt, 0].cpu().numpy()**2\n",
    ")\n",
    "\n",
    "target_vel_mag_series = np.sqrt(\n",
    "    target_u[traj_idx, :, x_pt, y_pt, 0].cpu().numpy()**2 + \n",
    "    target_v[traj_idx, :, x_pt, y_pt, 0].cpu().numpy()**2\n",
    ")\n",
    "\n",
    "# Time values for prediction window\n",
    "pred_times = np.arange(T_in, T_total) * dt\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.plot(pred_times, target_vel_mag_series, 'o-', label='Ground Truth', linewidth=2)\n",
    "ax.plot(pred_times, pred_vel_mag_series, 's--', label='Prediction', linewidth=2, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Time', fontsize=12)\n",
    "ax.set_ylabel('Velocity Magnitude', fontsize=12)\n",
    "ax.set_title(f'Time Evolution at Point ({x_pt}, {y_pt})', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Time series at spatial location ({x_pt}, {y_pt})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated how to use the Walrus foundation model for Navier-Stokes prediction:\n",
    "\n",
    "### What We Did:\n",
    "✓ Loaded Navier-Stokes spectral simulation data  \n",
    "✓ Downloaded and initialized the Walrus model (~1.3B parameters)  \n",
    "✓ Prepared data in the Walrus input format  \n",
    "✓ Ran autoregressive rollout for multi-step prediction  \n",
    "✓ Evaluated predictions using MSE and relative L2 error  \n",
    "✓ Visualized predictions vs ground truth  \n",
    "\n",
    "### Key Concepts:\n",
    "- **Autoregressive Rollout**: Predictions feed back as inputs for next timestep\n",
    "- **Field Indices**: Map physical fields to model embeddings\n",
    "- **Boundary Conditions**: Tell the model how the domain is bounded\n",
    "- **Padded Field Mask**: Distinguish real fields from padding\n",
    "- **Well Format**: Standardized structure for PDE data\n",
    "\n",
    "### Next Steps:\n",
    "1. **Fine-tuning**: Train the model on your specific dataset for better performance\n",
    "2. **Longer Rollouts**: Test prediction quality over longer time horizons\n",
    "3. **Different BCs**: Experiment with wall or open boundary conditions\n",
    "4. **Ensemble Predictions**: Run multiple predictions with different random seeds\n",
    "5. **Data Assimilation**: Incorporate new observations during rollout\n",
    "\n",
    "### Performance Notes:\n",
    "- Walrus is pretrained on diverse PDE datasets, so it may need fine-tuning for optimal performance on your specific Navier-Stokes data\n",
    "- The model uses reversible instance normalization (RevIN) to handle different scales\n",
    "- Predictions typically degrade over longer rollouts (error accumulation)\n",
    "- GPU acceleration is highly recommended for larger grids or longer rollouts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
