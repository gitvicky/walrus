{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Converting Data into Well Format\n",
    "\n",
    "While the next [example](./walrus_example_1_RunningWalrus.ipynb) shows how to use Walrus independently of our training code, our training and validation code will generally be easier to use if the data is formatted uniformly.\n",
    "\n",
    "For this, we give an example of how you would take a non-Well dataset [Bubble ML 2.0](https://arxiv.org/abs/2507.21244) and convert it into the Well format. We're specifically looking at the PoolBoilSubcooled data for R515B and FC72 that we used in the Walrus paper. Note that since this is for usage in Walrus where our goal is to infer a number of properties from history, we don't try to map every property stored in the dataset to the new format - just the ones we need. **If you use the BubbleML dataset in your work in the transformed format, please cite their paper.** Their project also contains a number of interesting scenarios, tasks, and forms of analysis that weren't used in Walrus.\n",
    "\n",
    "## What is The Well Format?\n",
    "\n",
    "The Well format is a standardized HDF5 structure for storing PDE simulation data. Key features:\n",
    "- **Hierarchical organization**: Dimensions, fields, boundary conditions, and scalars in separate groups\n",
    "- **Metadata-rich**: Attributes describe time-varying, sample-varying, and dimension-varying properties\n",
    "- **Field organization by rank**: Scalars (t0), vectors (t1), tensors (t2)\n",
    "- **Flexible**: Supports Cartesian, cylindrical, and spherical coordinates\n",
    "\n",
    "To start off let's make some paths to store our raw and processed data, then we'll download the data. The files we're going to be downloading are quite large, so it may be better to download them through your preferred approach (manual, huggingface CLI, or other) and point processed path to the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths for raw (downloaded) and processed (Well-formatted) data\n",
    "download_path = \"./raw_data/\"\n",
    "processed_path = \"./processed_data/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "os.makedirs(processed_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Download the BubbleML Data\n",
    "\n",
    "Note - this is a large dataset (~several GB), so you may want to download it independently and just point `download_path` at the download location.\n",
    "\n",
    "We'll download two materials:\n",
    "- **R515B**: A refrigerant commonly used in cooling systems\n",
    "- **FC72**: A dielectric fluid used in electronic cooling\n",
    "\n",
    "Each material has multiple HDF5 files (different wall temperatures) and accompanying JSON metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "# Map each HuggingFace subfolder to a local subdirectory\n",
    "# This organizes data by material type\n",
    "materials = {\n",
    "    \"PoolBoiling-Subcooled-R515B-2D\": os.path.join(download_path, \"R515B\"),\n",
    "    \"PoolBoiling-Subcooled-FC72-2D\":  os.path.join(download_path, \"FC72\"),\n",
    "}\n",
    "\n",
    "# Download files for each material\n",
    "for material, subdir in materials.items():\n",
    "    # Create subdirectory for this material\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "    \n",
    "    # Query HuggingFace API to get list of files in this folder\n",
    "    api = f\"https://huggingface.co/api/datasets/hpcforge/BubbleML_2/tree/main/{material}\"\n",
    "    resp = requests.get(api)\n",
    "    resp.raise_for_status()  # Raise error if request fails\n",
    "    items = resp.json()  # Parse JSON response\n",
    "\n",
    "    # Download each file in the folder\n",
    "    for item in items:\n",
    "        path = item.get(\"path\")\n",
    "        \n",
    "        # Skip if not a valid file path\n",
    "        if not path or item.get(\"type\") != \"file\":\n",
    "            continue\n",
    "        \n",
    "        # Construct download URL\n",
    "        url = f\"https://huggingface.co/datasets/hpcforge/BubbleML_2/resolve/main/{path}\"\n",
    "        out_file = os.path.join(subdir, os.path.basename(path))\n",
    "        \n",
    "        # Skip if file already exists\n",
    "        if os.path.exists(out_file):\n",
    "            print(\"skipping (exists):\", out_file)\n",
    "            continue\n",
    "        \n",
    "        # Download with streaming to handle large files\n",
    "        print(\"downloading:\", path, \"->\", out_file)\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(out_file, \"wb\") as f:\n",
    "                # Write in chunks (8KB at a time) for memory efficiency\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        print(\"saved:\", out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Understand the Well Format Structure\n",
    "\n",
    "If we look at the files we've downloaded, we have a collection of HDF5 and JSON files. To see what we're trying to create, let's look at the structure of a Well-formatted file.\n",
    "\n",
    "### Example: Helmholtz Staircase Dataset\n",
    "\n",
    "The overall file structure looks like:\n",
    "```\n",
    "<KeysViewHDF5 ['boundary_conditions', 'dimensions', 'scalars', 't0_fields', 't1_fields', 't2_fields']>\n",
    "{'dataset_name': 'helmholtz_staircase', 'grid_type': 'cartesian', 'n_spatial_dims': 2, 'n_trajectories': 26, 'omega': 0.06283032, 'simulation_parameters': array(['omega'], dtype=object)}\n",
    "├── boundary_conditions\n",
    "│   ├── x_open_neumann\n",
    "│   │   └── mask (1024,)              ← Boolean mask: 1 at boundary locations\n",
    "│   ├── xy_wall\n",
    "│   │   └── mask (1024, 256)\n",
    "│   └── y_open_neumann\n",
    "│       └── mask (256,)\n",
    "├── dimensions\n",
    "│   ├── time (50,)                     ← Temporal coordinate\n",
    "│   ├── x (1024,)                      ← Spatial coordinates\n",
    "│   └── y (256,)\n",
    "├── scalars\n",
    "│   └── omega ()                        ← Simulation parameters (0D)\n",
    "├── t0_fields                           ← Scalar fields (0-tensor)\n",
    "│   ├── mask (1024, 256)                  [spatial dims only]\n",
    "│   ├── pressure_im (26, 50, 1024, 256)   [traj, time, x, y]\n",
    "│   └── pressure_re (26, 50, 1024, 256)\n",
    "├── t1_fields                           ← Vector fields (1-tensor)\n",
    "│                                         [traj, time, x, y, components]\n",
    "└── t2_fields                           ← Tensor fields (2-tensor)\n",
    "                                          [traj, time, x, y, i, j]\n",
    "```\n",
    "\n",
    "### Key Observations:\n",
    "1. **Top-level groups**: `boundary_conditions`, `dimensions`, `scalars`, `t0_fields`, `t1_fields`, `t2_fields`\n",
    "2. **Metadata as attributes**: Dataset-level info stored in HDF5 attributes\n",
    "3. **Field organization by tensor rank**: \n",
    "   - t0 = scalars (temperature, pressure)\n",
    "   - t1 = vectors (velocity)\n",
    "   - t2 = tensors (stress, strain)\n",
    "4. **Consistent ordering**: `[trajectory, time, x, y, z, components]`\n",
    "\n",
    "So for BubbleML data, we want to define these parameters based on the content of the HDF5 files and their associated metadata. We'll define a quick little helper to view the HDF5 structure once we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def h5_tree(val, pre=''):\n",
    "    \"\"\"Recursively print HDF5 file structure in a tree format.\n",
    "    \n",
    "    Args:\n",
    "        val: HDF5 group or file object\n",
    "        pre: Prefix string for tree formatting (for recursion)\n",
    "    \n",
    "    This is a helpful utility for visualizing HDF5 file structure.\n",
    "    \"\"\"\n",
    "    items = len(val)\n",
    "    for key, val in val.items():\n",
    "        items -= 1\n",
    "        \n",
    "        # Determine if this is the last item (affects tree formatting)\n",
    "        if items == 0:\n",
    "            # Last item - use └── symbol\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                # It's a group - recurse into it\n",
    "                print(pre + f'└── {key}')\n",
    "                h5_tree(val, pre+'    ')  # Add 4 spaces for indentation\n",
    "            else:\n",
    "                # It's a dataset - show its shape\n",
    "                try:\n",
    "                    print(pre + f'└── {key} {val.shape}')\n",
    "                except TypeError:\n",
    "                    # Scalar dataset (no shape)\n",
    "                    print(pre + f'└── {key} (scalar)')\n",
    "        else:\n",
    "            # Not the last item - use ├── symbol\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                print(pre + f'├── {key}')\n",
    "                h5_tree(val, pre+'│   ')  # Add │ for vertical line\n",
    "            else:\n",
    "                try:\n",
    "                    print(pre + f'├── {key} {val.shape}')\n",
    "                except TypeError:\n",
    "                    print(pre + f'├── {key} (scalar)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Examine the Source Data Structure\n",
    "\n",
    "Now let's look at the structure of the BubbleML data we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at an example JSON file\n",
    "# This contains simulation parameters and configuration\n",
    "!cat {download_path}/R515B/Twall_100.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the HDF5 structure\n",
    "# This contains the actual simulation data (fields)\n",
    "example_bubble = h5py.File(f\"{download_path}/R515B/Twall_100.hdf5\", 'r')\n",
    "print(\"BubbleML HDF5 Structure:\")\n",
    "print(\"=\" * 50)\n",
    "h5_tree(example_bubble)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Key Observations:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"- Shape: (2001, 512, 512) = (time, y, x)\")\n",
    "print(\"  Note: Python uses (y, x) order for images\")\n",
    "print(\"  Well format uses (x, y) order - we'll need to transpose!\")\n",
    "print(\"\\n- Fields available:\")\n",
    "print(\"  • dfun: Distance function (gas-liquid interface SDF)\")\n",
    "print(\"  • temperature: Temperature field\")\n",
    "print(\"  • velx, vely: Velocity components\")\n",
    "print(\"  • pressure: Pressure field\")\n",
    "print(\"  • massflux: Mass flux\")\n",
    "print(\"  • normx, normy: Interface normals\")\n",
    "print(\"\\n- Coordinates:\")\n",
    "print(\"  • x_centers, y_centers: Cell centers\")\n",
    "print(\"  • x_faces, y_faces: Cell faces (one extra point)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Define the Transformation Function\n",
    "\n",
    "We can see that each HDF5 file appears to be one trajectory generated under a given configuration defined by the JSON file. The trajectory is 2001 steps long and contains snapshots from a 512x512 field which we will place at the volume centers.\n",
    "\n",
    "### Fields to Include\n",
    "\n",
    "Trying to match the settings used in the Bubbleformer paper, we'll include:\n",
    "- `dfun` (gas-liquid interface SDF)\n",
    "- `temperature`\n",
    "- `velocity` (both x and y components)\n",
    "\n",
    "We won't try to include all of the information - Walrus is supposed to try to infer this from the history after all - but we'll add some of the high level details to the file as an example of how you would do this.\n",
    "\n",
    "### Important Note on Axis Order\n",
    "\n",
    "For Euclidean data, the Well canonically stores data in **[x, y, z]** order. \n",
    "2D images in Python are usually stored in **[y, x]** format, so we'll need to **transpose** these axes to match other data.\n",
    "\n",
    "### Boundary Condition Mapping\n",
    "\n",
    "Boundaries are the main tricky element. BubbleML uses:\n",
    "- **No-slip conditions**: Models contact between fluid and solid surface → maps to `\"WALL\"`\n",
    "- **Outflow BCs**: Flow can continue past this point → maps to `\"OPEN\"`\n",
    "\n",
    "The Well uses topological descriptors:\n",
    "- `\"WALL\"`: Closed/reflective boundary (solid surface)\n",
    "- `\"OPEN\"`: Open boundary (inflow/outflow)\n",
    "- `\"PERIODIC\"`: Wrapping boundary (toroidal topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def translate_bubble(hdf5_path, json_path, subname, out_path):\n",
    "    \"\"\"Convert a single BubbleML HDF5 file to Well format.\n",
    "    \n",
    "    Args:\n",
    "        hdf5_path: Path to source HDF5 file (contains field data)\n",
    "        json_path: Path to corresponding JSON file (contains metadata)\n",
    "        subname: Material name (e.g., 'R515B', 'FC72')\n",
    "        out_path: Output directory for Well-formatted file\n",
    "    \n",
    "    The function performs these key transformations:\n",
    "    1. Extracts fields from source HDF5\n",
    "    2. Transposes spatial axes from (y, x) to (x, y)\n",
    "    3. Organizes into Well format groups\n",
    "    4. Adds appropriate metadata attributes\n",
    "    \"\"\"\n",
    "    # Extract temperature from filename (e.g., \"Twall_100.hdf5\" -> 100)\n",
    "    baseT = int(hdf5_path.split('_')[-1].replace('.hdf5',''))\n",
    "    \n",
    "    # Load JSON metadata\n",
    "    json_file = json.load(open(json_path))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Extract and transform field data from source HDF5\n",
    "    # ========================================================================\n",
    "    with h5py.File(hdf5_path, 'r') as f:\n",
    "        # Extract gas-liquid interface distance function\n",
    "        # Original: (T, Y, X) -> swapaxes(1,2) -> (T, X, Y) -> add batch dim -> (1, T, X, Y)\n",
    "        gas_interface_sdf = np.swapaxes(f['dfun'][:], 1, 2)[None]\n",
    "        \n",
    "        # Extract velocity components and transpose\n",
    "        # Original: (T, Y, X) -> (T, X, Y) -> (1, T, X, Y)\n",
    "        vx = np.swapaxes(f['velx'][:], 1, 2)[None]\n",
    "        vy = np.swapaxes(f['vely'][:], 1, 2)[None]\n",
    "        # Stack into velocity vector: (1, T, X, Y, 2)\n",
    "        vel = np.stack([vx, vy], -1)\n",
    "        \n",
    "        # Extract temperature field\n",
    "        temp = np.swapaxes(f['temperature'][:], 1, 2)[None]\n",
    "        \n",
    "        # Extract spatial coordinates\n",
    "        # Use cell centers (not faces) for field values\n",
    "        x_coords = f['x_centers'][:]\n",
    "        y_coords = f['y_centers'][:]\n",
    "        \n",
    "        # Create time coordinate array\n",
    "        # 2001 timesteps from 0 to t_final\n",
    "        t = np.linspace(0, json_file[\"t_final\"], 2001, endpoint=True)\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 2: Create Well-formatted HDF5 file\n",
    "    # ========================================================================\n",
    "    outpath = out_path + f\"bubbleML_PoolBoiling-Subcooled_{subname}_T{baseT}.hdf5\"\n",
    "    with h5py.File(outpath, 'w') as f:\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 2a. Top-level metadata (stored as file attributes)\n",
    "        # ====================================================================\n",
    "        f.attrs[\"dataset_name\"] = f\"bubbleML_PoolBoiling-Subcooled\"\n",
    "        \n",
    "        # Grid type determines which augmentations can be applied\n",
    "        # Options: 'cartesian', 'cylindrical', 'spherical'\n",
    "        f.attrs[\"grid_type\"] = \"cartesian\"\n",
    "        \n",
    "        f.attrs[\"n_spatial_dims\"] = 2  # 2D simulation\n",
    "        f.attrs[\"n_trajectories\"] = 1  # Each file contains one trajectory\n",
    "\n",
    "        # Store simulation parameters used in BubbleML benchmarks\n",
    "        # These describe the physical properties and numerical settings\n",
    "        f.attrs[\"simulation_parameters\"] = [\n",
    "            \"inv_reynolds\",       # 1/Reynolds number (viscosity)\n",
    "            \"cpgas\",              # Specific heat capacity of gas\n",
    "            \"mugas\",              # Dynamic viscosity of gas\n",
    "            \"rhogas\",             # Density of gas\n",
    "            \"thcogas\",            # Thermal conductivity of gas\n",
    "            \"stefan\",             # Stefan number (phase change)\n",
    "            \"prandtl\",            # Prandtl number (momentum/thermal diffusion)\n",
    "            \"heater-nucWaitTime\", # Nucleation wait time\n",
    "            \"heater-wallTemp\"     # Wall temperature\n",
    "        ]\n",
    "        \n",
    "        # Store each parameter value as an attribute\n",
    "        f.attrs[\"inv_reynolds\"] = json_file[\"inv_reynolds\"]\n",
    "        f.attrs[\"cpgas\"] = json_file[\"cpgas\"]\n",
    "        f.attrs[\"mugas\"] = json_file[\"mugas\"]\n",
    "        f.attrs[\"rhogas\"] = json_file[\"rhogas\"]\n",
    "        f.attrs[\"thcogas\"] = json_file[\"thcogas\"]\n",
    "        f.attrs[\"stefan\"] = json_file[\"stefan\"]\n",
    "        f.attrs[\"prandtl\"] = json_file[\"prandtl\"]\n",
    "        f.attrs[\"heater-nucWaitTime\"] = json_file[\"heater\"][\"nucWaitTime\"]\n",
    "        f.attrs[\"heater-wallTemp\"] = json_file[\"heater\"][\"wallTemp\"]\n",
    "\n",
    "        # ====================================================================\n",
    "        # 2b. Spatial/temporal dimensions\n",
    "        # ====================================================================\n",
    "        dims = f.create_group(\"dimensions\")\n",
    "        dims.attrs[\"spatial_dims\"] = [\"x\", \"y\"]  # Dimension names\n",
    "        \n",
    "        # Time dimension\n",
    "        time = dims.create_dataset(\"time\", data=t)\n",
    "        time.attrs[\"time_varying\"] = True   # Time varies over time (always true for time)\n",
    "        time.attrs[\"sample_varying\"] = False  # Same time grid for all trajectories\n",
    "        \n",
    "        # X spatial dimension\n",
    "        x = dims.create_dataset(\"x\", data=x_coords)\n",
    "        x.attrs[\"time_varying\"] = False  # Fixed grid (not a moving mesh)\n",
    "        x.attrs[\"sample_varying\"] = False  # Same grid for all trajectories\n",
    "        \n",
    "        # Y spatial dimension\n",
    "        y = dims.create_dataset(\"y\", data=y_coords)\n",
    "        y.attrs[\"time_varying\"] = False\n",
    "        y.attrs[\"sample_varying\"] = False\n",
    "\n",
    "        # ====================================================================\n",
    "        # 2c. Boundary conditions\n",
    "        # ====================================================================\n",
    "        # BCs are defined as masks indicating where each BC type applies\n",
    "        boundaries = f.create_group(\"boundary_conditions\")\n",
    "        \n",
    "        # X-direction: No-slip walls on both sides\n",
    "        x_wall_noslip = boundaries.create_group(\"x_wall_noslip\")\n",
    "        xmask = np.zeros(x_coords.shape[0])  # Start with all zeros\n",
    "        xmask[0] = 1   # Mark first index (lower boundary)\n",
    "        xmask[-1] = 1  # Mark last index (upper boundary)\n",
    "        x_wall_noslip.create_dataset(\"mask\", data=xmask, dtype=np.int8)\n",
    "        x_wall_noslip.attrs[\"bc_type\"] = \"WALL\"  # Solid wall\n",
    "        x_wall_noslip.attrs[\"associated_dims\"] = [\"x\"]  # Applies to x-dimension\n",
    "        x_wall_noslip.attrs[\"associated_fields\"] = []  # No specific fields\n",
    "        x_wall_noslip.attrs[\"sample_varying\"] = False\n",
    "        x_wall_noslip.attrs[\"time_varying\"] = False\n",
    "        \n",
    "        # Y-direction lower: No-slip wall (heated surface)\n",
    "        y_wall_noslip = boundaries.create_group(\"y_wall_noslip\")\n",
    "        ymask = np.zeros(y_coords.shape[0])\n",
    "        ymask[0] = 1  # Only lower boundary is a wall\n",
    "        y_wall_noslip.create_dataset(\"mask\", data=ymask, dtype=np.int8)\n",
    "        y_wall_noslip.attrs[\"bc_type\"] = \"WALL\"\n",
    "        y_wall_noslip.attrs[\"associated_dims\"] = [\"y\"]\n",
    "        y_wall_noslip.attrs[\"associated_fields\"] = []\n",
    "        y_wall_noslip.attrs[\"sample_varying\"] = False\n",
    "        y_wall_noslip.attrs[\"time_varying\"] = False\n",
    "\n",
    "        # Y-direction upper: Open (outflow)\n",
    "        y_open = boundaries.create_group(\"y_open\")\n",
    "        ymask = np.zeros(y_coords.shape[0])\n",
    "        ymask[-1] = 1  # Only upper boundary is open\n",
    "        y_open.create_dataset(\"mask\", data=ymask, dtype=np.int8)\n",
    "        y_open.attrs[\"bc_type\"] = \"OPEN\"  # Outflow boundary\n",
    "        y_open.attrs[\"associated_dims\"] = [\"y\"]\n",
    "        y_open.attrs[\"associated_fields\"] = []\n",
    "        y_open.attrs[\"sample_varying\"] = False\n",
    "        y_open.attrs[\"time_varying\"] = False\n",
    "\n",
    "        # ====================================================================\n",
    "        # 2d. Scalars (simulation parameters as datasets)\n",
    "        # ====================================================================\n",
    "        # These are 0-dimensional quantities (single numbers)\n",
    "        # For most use cases in Walrus, these are unused metadata\n",
    "        scalars = f.create_group(\"scalars\")\n",
    "        scalars.attrs[\"field_names\"] = [\n",
    "            \"inv_reynolds\", \"cpgas\", \"mugas\", \"rhogas\", \"thcogas\",\n",
    "            \"stefan\", \"prandtl\", \"heater-nucWaitTime\", \"heater-wallTemp\"\n",
    "        ]\n",
    "        \n",
    "        # Create a dataset for each scalar parameter\n",
    "        # These are all constant (don't vary with time or trajectory)\n",
    "        for param_name in [\"inv_reynolds\", \"cpgas\", \"mugas\", \"rhogas\", \"thcogas\",\n",
    "                           \"stefan\", \"prandtl\"]:\n",
    "            dset = scalars.create_dataset(param_name, data=json_file[param_name])\n",
    "            dset.attrs[\"sample_varying\"] = False\n",
    "            dset.attrs[\"time_varying\"] = False\n",
    "        \n",
    "        # Heater parameters come from nested JSON structure\n",
    "        heater_nucWaitTime = scalars.create_dataset(\n",
    "            \"heater-nucWaitTime\", data=json_file[\"heater\"][\"nucWaitTime\"]\n",
    "        )\n",
    "        heater_nucWaitTime.attrs[\"sample_varying\"] = False\n",
    "        heater_nucWaitTime.attrs[\"time_varying\"] = False\n",
    "\n",
    "        heater_wallTemp = scalars.create_dataset(\n",
    "            \"heater-wallTemp\", data=json_file[\"heater\"][\"wallTemp\"]\n",
    "        )\n",
    "        heater_wallTemp.attrs[\"sample_varying\"] = False\n",
    "        heater_wallTemp.attrs[\"time_varying\"] = False\n",
    "\n",
    "        # ====================================================================\n",
    "        # 2e. Fields - THE ACTUAL DATA WE'RE MODELING\n",
    "        # ====================================================================\n",
    "        \n",
    "        # t0_fields: Scalar (0-tensor) fields\n",
    "        # Shape: [trajectory, time, x, y]\n",
    "        t0 = f.create_group(\"t0_fields\")\n",
    "        t0.attrs[\"field_names\"] = [\"gas-interface-sdf\", \"temperature\"]\n",
    "        \n",
    "        # Gas-liquid interface signed distance function\n",
    "        # Positive = gas phase, negative = liquid phase, zero = interface\n",
    "        sdf_dset = t0.create_dataset(\"gas-interface-sdf\", data=gas_interface_sdf, dtype=np.float32)\n",
    "        sdf_dset.attrs[\"dim_varying\"] = np.array([True, True])  # Varies over both x and y\n",
    "        sdf_dset.attrs[\"sample_varying\"] = True   # Different for each trajectory\n",
    "        sdf_dset.attrs[\"time_varying\"] = True     # Evolves over time\n",
    "\n",
    "        # Temperature field\n",
    "        temp_dset = t0.create_dataset(\"temperature\", data=temp, dtype=np.float32)\n",
    "        temp_dset.attrs[\"dim_varying\"] = np.array([True, True])\n",
    "        temp_dset.attrs[\"sample_varying\"] = True\n",
    "        temp_dset.attrs[\"time_varying\"] = True\n",
    "\n",
    "        # t1_fields: Vector (1-tensor) fields\n",
    "        # Shape: [trajectory, time, x, y, components]\n",
    "        # Components dimension has size = n_spatial_dims (2 for 2D velocity)\n",
    "        t1 = f.create_group(\"t1_fields\")\n",
    "        t1.attrs[\"field_names\"] = [\"velocity\"]\n",
    "        \n",
    "        # Velocity vector field (v_x, v_y)\n",
    "        v_dset = t1.create_dataset(\"velocity\", data=vel, dtype=np.float32)\n",
    "        v_dset.attrs[\"dim_varying\"] = np.array([True, True])  # Varies over x and y\n",
    "        v_dset.attrs[\"sample_varying\"] = True\n",
    "        v_dset.attrs[\"time_varying\"] = True\n",
    "\n",
    "        # t2_fields: Tensor (2-tensor) fields\n",
    "        # Shape: [trajectory, time, x, y, i, j]\n",
    "        # Would include stress tensors, strain tensors, etc.\n",
    "        # We don't have any 2-tensor fields in this dataset\n",
    "        t2 = f.create_group(\"t2_fields\")\n",
    "        t2.attrs[\"field_names\"] = []\n",
    "    \n",
    "    print(f\"✓ Created: {outpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 5: Convert All Files\n",
    "\n",
    "Now that we have our files and our transformation definition, let's apply the conversion to all downloaded data!\n",
    "\n",
    "We'll organize the output into standard train/valid/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing processed data\n",
    "!rm -rf {processed_path}/data/\n",
    "\n",
    "# Create directory structure for train/valid/test splits\n",
    "!mkdir -p {processed_path}/data/train/\n",
    "!mkdir -p {processed_path}/data/valid/\n",
    "!mkdir -p {processed_path}/data/test/\n",
    "\n",
    "# Process both materials\n",
    "materials = [\"FC72\", \"R515B\"]\n",
    "\n",
    "print(\"Converting BubbleML data to Well format...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for material in materials:\n",
    "    print(f\"\\nProcessing {material}...\")\n",
    "    \n",
    "    # Get all HDF5 files for this material\n",
    "    base_path = os.path.join(download_path, material)\n",
    "    out_path = f\"{processed_path}/data/train/\"\n",
    "    files = glob.glob(base_path + '/*.hdf5')\n",
    "    \n",
    "    # Convert each file\n",
    "    for file in files:\n",
    "        hdf5_path = file\n",
    "        json_path = file.replace('.hdf5', '.json')\n",
    "        \n",
    "        # Verify both files exist\n",
    "        if not (os.path.exists(hdf5_path) and os.path.exists(json_path)):\n",
    "            print(f\"  ⚠ Warning: Missing files for {file}\")\n",
    "            continue\n",
    "        \n",
    "        # Perform conversion\n",
    "        translate_bubble(hdf5_path, json_path, material, out_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 6: Create Validation and Test Splits\n",
    "\n",
    "The files should now be transformed! The problem now is we don't have separate validation and test sets yet.\n",
    "\n",
    "In the experiments in the Walrus paper, only 2 wall temperatures were used for validation/testing per material, so we'll repeat this pattern here. \n",
    "\n",
    "**Note**: If you're doing extensive hyperparameter tuning on validation, you'll generally want to avoid having identical validation and test sets, but we're using this pattern to match the original experimental settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which files to use for validation and test\n",
    "# These are specific wall temperatures chosen to be held out\n",
    "valid_files = [\n",
    "    \"bubbleML_PoolBoiling-Subcooled_FC72_T107.hdf5\",   # FC72 at 107°C\n",
    "    \"bubbleML_PoolBoiling-Subcooled_FC72_T97.hdf5\",    # FC72 at 97°C\n",
    "    \"bubbleML_PoolBoiling-Subcooled_R515B_T30.hdf5\",   # R515B at 30°C\n",
    "    \"bubbleML_PoolBoiling-Subcooled_R515B_T40.hdf5\"    # R515B at 40°C\n",
    "]\n",
    "\n",
    "# Use same files for test (following original paper)\n",
    "test_files = [\n",
    "    \"bubbleML_PoolBoiling-Subcooled_FC72_T107.hdf5\",\n",
    "    \"bubbleML_PoolBoiling-Subcooled_FC72_T97.hdf5\",\n",
    "    \"bubbleML_PoolBoiling-Subcooled_R515B_T30.hdf5\",\n",
    "    \"bubbleML_PoolBoiling-Subcooled_R515B_T40.hdf5\"\n",
    "]\n",
    "\n",
    "print(\"Creating validation and test splits...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Move validation files from train to valid directory\n",
    "print(\"\\nMoving validation files...\")\n",
    "for vf in valid_files:\n",
    "    src = f\"{processed_path}/data/train/{vf}\"\n",
    "    dst = f\"{processed_path}/data/valid/{vf}\"\n",
    "    if os.path.exists(src):\n",
    "        !mv {src} {dst}\n",
    "        print(f\"  ✓ {vf}\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Warning: {vf} not found\")\n",
    "\n",
    "# Copy validation files to test directory\n",
    "print(\"\\nCopying test files...\")\n",
    "for tf in test_files:\n",
    "    src = f\"{processed_path}/data/valid/{tf}\"\n",
    "    dst = f\"{processed_path}/data/test/{tf}\"\n",
    "    if os.path.exists(src):\n",
    "        !cp {src} {dst}\n",
    "        print(f\"  ✓ {tf}\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Warning: {tf} not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Data split complete!\")\n",
    "print(f\"\\nFinal structure:\")\n",
    "print(f\"  train/: {len(os.listdir(processed_path + '/data/train/'))} files\")\n",
    "print(f\"  valid/: {len(os.listdir(processed_path + '/data/valid/'))} files\")\n",
    "print(f\"  test/:  {len(os.listdir(processed_path + '/data/test/'))} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 7: Verify the Conversion\n",
    "\n",
    "Perfect! The Well contains a number of utilities to help verify transformed data. In particular, there is the [format checker tool](https://github.com/PolymathicAI/the_well/blob/master/scripts/check_thewell_formatting.py).\n",
    "\n",
    "However, here we're going to assume the formatting is OK and verify by loading the data directly with Walrus's dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from walrus.data.inflated_dataset import BatchInflatedWellDataset\n",
    "\n",
    "# Load the training dataset\n",
    "# This will read all HDF5 files in the train/ directory\n",
    "print(\"Loading Well-formatted dataset...\")\n",
    "dataset = BatchInflatedWellDataset(\n",
    "    path=f\"{processed_path}\",          # Base path containing data/ folder\n",
    "    well_split_name=\"train\",           # Use train split\n",
    "    normalization_type=None            # No normalization for now\n",
    ")\n",
    "\n",
    "print(f\"✓ Dataset loaded successfully!\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Fields available: {dataset.metadata.field_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 8: Visualize a Sample\n",
    "\n",
    "Let's load a sample and visualize the temperature field to verify everything looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load sample 1000 from the dataset\n",
    "samples = dataset[1000]\n",
    "\n",
    "print(\"Sample structure:\")\n",
    "print(f\"  Keys: {samples.keys()}\")\n",
    "print(f\"  input_fields shape: {samples['input_fields'].shape}\")\n",
    "print(f\"  Field order: {dataset.metadata.field_names}\")\n",
    "\n",
    "# Extract temperature field from first timestep\n",
    "# Shape: [time, x, y, fields]\n",
    "# Temperature is field index 1 (after gas-interface-sdf)\n",
    "temperature = samples[\"input_fields\"][0, ..., 1].squeeze()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "img = plt.imshow(temperature.T, origin='lower', cmap='hot')  # Transpose for visualization\n",
    "plt.colorbar(img, label='Temperature')\n",
    "plt.title('Temperature Field - BubbleML Pool Boiling\\n(Sample 1000, t=0)', fontsize=14)\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization complete!\")\n",
    "print(\"  You should see the heated bottom boundary and cooler fluid above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully converted the BubbleML dataset to Well format.\n",
    "\n",
    "### What we did:\n",
    "1. ✓ Downloaded BubbleML data from HuggingFace\n",
    "2. ✓ Examined source data structure (HDF5 + JSON)\n",
    "3. ✓ Defined transformation function to Well format\n",
    "4. ✓ Handled axis transposition (y,x) → (x,y)\n",
    "5. ✓ Mapped boundary conditions (no-slip → WALL, outflow → OPEN)\n",
    "6. ✓ Organized fields by tensor rank (t0, t1, t2)\n",
    "7. ✓ Created train/valid/test splits\n",
    "8. ✓ Verified with Walrus dataset loader\n",
    "\n",
    "### Key takeaways:\n",
    "- **Well format is hierarchical**: dimensions, boundaries, scalars, fields\n",
    "- **Fields organized by rank**: t0 (scalars), t1 (vectors), t2 (tensors)\n",
    "- **Metadata is crucial**: Attributes describe varying properties\n",
    "- **Axis order matters**: Well uses (x, y, z), Python images use (y, x)\n",
    "- **BCs are topological**: WALL, OPEN, PERIODIC describe connectivity\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "**To use this in a Walrus model**, check out the next [notebook](walrus_example_1_RunningWalrus.ipynb).\n",
    "\n",
    "**To define a config file** for use in the full Walrus training codebase, check out the config file [here](https://github.com/PolymathicAI/walrus/blob/walrus/configs/data/bubbleml_poolboil_subcool.yaml).\n",
    "\n",
    "**To convert your own data**, follow this pattern:\n",
    "1. Examine your source data structure\n",
    "2. Map your fields to Well's t0/t1/t2 groups\n",
    "3. Map your boundary conditions to WALL/OPEN/PERIODIC\n",
    "4. Transpose axes to match (x, y, z) order\n",
    "5. Add appropriate metadata attributes\n",
    "6. Verify with format checker or dataset loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
